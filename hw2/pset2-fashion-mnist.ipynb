{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import matplotlib as mpl\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "#Fashion-MNIST classes (mapping from class index to class name)\n",
    "class_names = ['tshirt/top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "# some utilities\n",
    "\n",
    "def create_submission_file(fname, preds):\n",
    "    \"\"\"\n",
    "    Create Kaggle submision with predictions written as a csv (comma separated values) file \n",
    "    \"\"\"\n",
    "    \n",
    "    ofile  = open(fname, \"w\")\n",
    "    writer = csv.writer(ofile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)    \n",
    "    \n",
    "    writer.writerow(['id', 'label'])\n",
    "    \n",
    "    \n",
    "    for i in range(preds.shape[0]):\n",
    "        writer.writerow([i,preds[i]])\n",
    "        \n",
    "\n",
    "        \n",
    "def read_fMNIST(dataset = \"training\", path = \"./\", load_small=False):\n",
    "    \"\"\"\n",
    "    reading in the \"Fashion MNIST\" data\n",
    "    this function allows specification of the part to be read (training/testing/validation)\n",
    "    if load_small = True, this will look for the file(s) associated with the small training set\n",
    "    Note that if dataset='testing', no labels will be returned\n",
    "    \"\"\"\n",
    "    \n",
    "    #Figure out the name of the file to load    \n",
    "    if dataset.lower() == \"training\":\n",
    "        file_name_suffix = 'Tr'\n",
    "        has_labels = True\n",
    "    \n",
    "    elif dataset.lower() == \"validation\":\n",
    "        file_name_suffix = 'Vl'\n",
    "        has_labels = True\n",
    "\n",
    "    elif dataset.lower() == \"testing\":\n",
    "        file_name_suffix = 'Te'\n",
    "        has_labels = False\n",
    "\n",
    "    else:\n",
    "        print(\"dataset must be 'testing','validation', or 'training'\")\n",
    "        raise ValueError\n",
    "    \n",
    "    if load_small:\n",
    "        file_name_suffix += '_sm'\n",
    "    \n",
    "    #Load the appropriate files\n",
    "    X = np.load('./x'+file_name_suffix+'.npy')\n",
    "    if has_labels:\n",
    "        y = np.load('./y'+file_name_suffix+'.npy')\n",
    "    \n",
    "\n",
    "    #Return the appropriate data\n",
    "    if has_labels:\n",
    "        return X,y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "def show_fMNIST_example(image):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 2D array of pixel data.\n",
    "    \"\"\"\n",
    "    image = image[-28**2:]\n",
    "    image = image.reshape(28,28)\n",
    "\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.axis('off')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X,y = read_fMNIST('training')\n",
    "print('Training images')\n",
    "print('Class: %s'%class_names[np.argmax(y[10])])\n",
    "show_fMNIST_example(X[10])\n",
    "print('Class: %s'%class_names[np.argmax(y[20])])\n",
    "show_fMNIST_example(X[20])\n",
    "\n",
    "\n",
    "X,y = read_fMNIST('validation')\n",
    "print('Validation images')\n",
    "print('Class: %s'%class_names[np.argmax(y[10])])\n",
    "show_fMNIST_example(X[10])\n",
    "print('Class: %s'%class_names[np.argmax(y[20])])\n",
    "show_fMNIST_example(X[20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the code for our softmax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(scores):\n",
    "    \"\"\"\n",
    "    takes the softmax along the second dimension of a matrix, returns class scores\n",
    "    \"\"\"\n",
    "    # we will adjust the dynamic range by subtracting the max, to prevent potential underflow in exp\n",
    "    exp_scores = np.exp(scores-np.max(scores,axis=1,keepdims=True))+1e-6\n",
    "    return exp_scores/(np.sum(exp_scores,axis=1,keepdims=True))\n",
    "\n",
    "\n",
    "def log_likelihood(X,w,y):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "      X: a Nxd matrix where each of the N rows is a datapont with d features\n",
    "      Y: a Nx10 matrix, each row is a one-hot vector with the ith entry is 1 if that datapoint belongs to class i \n",
    "      w: is a dx1 matrix containing your model parameters\n",
    "    Outputs:\n",
    "      LL: a scalar containing the AVERAGE log-likelihood of the dataset's labels y, given inputs X and model w\n",
    "    \"\"\"\n",
    "    #compute the un-normalized 'scores' of each class for each datapoint\n",
    "    scores = infer(X,w)\n",
    "    \n",
    "    #normalize the scores to get a distribution over classes for each datapoint\n",
    "    predictions = ************* your code\n",
    "        \n",
    "    #Use the predicted distributions, and the true distributions to compute log-likelihood\n",
    "    LL = *********** your code\n",
    "\n",
    "    return LL\n",
    "\n",
    "def objective(X,w,y):\n",
    "    \"\"\"\n",
    "    Compute components of the optimization objective\n",
    "    Output:\n",
    "    logloss : value of log-loss (negative average log-likelihood)\n",
    "    regularizer: value of the regularization term (Frobenius norm of w), NOT multiplied by lambda\n",
    "    \"\"\"\n",
    "    regularizer = np.linalg.norm(w,'fro')\n",
    "    logloss = - log_likelihood(X,w,y)\n",
    "    return logloss, regularizer\n",
    "\n",
    "\n",
    "\n",
    "def gradient(X,w,y):\n",
    "    \"\"\"\n",
    "    Compute in grad the gradient of the log-loss of the model (w) given the features (X) and labels (y) w.r.t. w\n",
    "    Also compute and return in l2grad the gradient of the regularizer lambda*norm(w)\n",
    "    Note: we can't use the name 'lambda' since it's a keyword in Python\n",
    "    Reminder: the full objective is -log p(y|X;w) + lmbda*norm2(w)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Compute any extra variable needed to compute the gradient:\n",
    "    scores = np.dot(X,w)\n",
    "    predictions = softmax(scores)\n",
    "\n",
    "    #Compute the gradient of the average log-loss\n",
    "    grad = ************ your code\n",
    "    \n",
    "    l2grad = ****************** your code\n",
    "    \n",
    "    return grad, l2grad\n",
    "\n",
    "def infer(X,w):\n",
    "    \"\"\"\n",
    "    Compute the class scores that model w gives for each class, given data X\n",
    "    \"\"\"\n",
    "    scores = np.dot(X,w)\n",
    "    return scores\n",
    "\n",
    "def accuracy(X,w,y):\n",
    "    \"\"\"\n",
    "    Compute accuracy (one minus average 0/1 loss) of model w relative to true labels y on data X\n",
    "    \"\"\"\n",
    "    y_hat = np.argmax(infer(X,w),axis=1)\n",
    "    y = np.argmax(y,axis=1)\n",
    "    acc = np.sum(np.equal(y,y_hat).astype(np.float32))/y.shape[0]\n",
    "    return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the SGD (stochastic gradient descent) procedure for softmax, with optional regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch_sgd(xTr, yTr, xVl, yVl, lmbda=0, lr = .01,maxepochs=10,bsize=32,objTol=.01,verbose=1):\n",
    "    \n",
    "    # initialization of w (feel free to play with this)\n",
    "    w = np.zeros((xTr.shape[1],yTr.shape[1]))\n",
    "    \n",
    "    old_obj = 1e9  \n",
    "    old_w = w\n",
    "    \n",
    "    # we will organize the run in terms of epochs (one epcoh = one full pass over data)\n",
    "    # to keep track of learning curves, allocate space\n",
    "    trAcc = np.zeros(maxepochs)\n",
    "    obj = np.zeros(maxepochs)\n",
    "    \n",
    "    for epoch in range(maxepochs):\n",
    "        \n",
    "        #shuffle the data\n",
    "        index = np.random.permutation(xTr.shape[0])\n",
    "        nBatches = np.floor(xTr.shape[0]/bsize).astype(np.integer) # ignore the remainder N-floor(N/bsize), for simplicity\n",
    "    \n",
    "        for b in range(nBatches):\n",
    "            # fill in the batch\n",
    "            iBatch = index[b*bsize:(b+1)*bsize]\n",
    "            xBatch = xTr[iBatch]\n",
    "            yBatch = yTr[iBatch]\n",
    "            \n",
    "            grad,l2grad = gradient(xBatch,w,yBatch)\n",
    "    \n",
    "            w = w-lr*(grad+lmbda*l2grad)\n",
    "            \n",
    "        \n",
    "        # end of an epoch: test for convergence by looking at validation\n",
    "        logloss, regularizer = objective(xTr,w,yTr)\n",
    "        obj[epoch] = logloss+lmbda*regularizer\n",
    "                \n",
    "        # also record accuracy on training\n",
    "        trAcc[epoch] = accuracy(xTr,w,yTr)\n",
    "\n",
    "        obj_gain = (old_obj-obj[epoch])/np.abs(old_obj) # improvement in training objective this epich\n",
    "        \n",
    "        if verbose > 0.5:\n",
    "            print('Epoch %d: obj=%.4f, gain %.4f  [train acc %.4f]'%(epoch,obj[epoch],obj_gain,trAcc[epoch]))\n",
    "        \n",
    "        \n",
    "        if epoch == maxepochs-1:\n",
    "            if verbose > 0:\n",
    "                print('Reached max epochs, stopping')\n",
    "            return w, obj, trAcc\n",
    "        \n",
    "        if obj_gain < objTol:  \n",
    "            lr = lr/2\n",
    "            if verbose > 0:\n",
    "                print('Dropping learning rate to %.4f'%lr)\n",
    "        \n",
    "        # update bookkeeping before going to next epoch\n",
    "        old_obj = obj[epoch]    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start experiments, we need one more piece: feature transform. In this case we are using the \"raw\" features (pixel values) plus the constant term, but with normalization that applies z-scoring to each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if mu and sigma are provided, they are used to normalize each pixel\n",
    "# otherwise, they are computed and returned\n",
    "def preprocess_data(X,mu = None,sigma = None,visualize=False):\n",
    "\n",
    "    if visualize:\n",
    "        print('VISUALIZE BEFORE NORMALIZATION')\n",
    "        #Visualize example before normalization\n",
    "        show_fMNIST_example(X[300])\n",
    "        show_fMNIST_example(X[500])\n",
    "    \n",
    "    if mu is None: # need to compute normalizing stats\n",
    "        compute_stats = True\n",
    "        mu = X.mean(axis=0, keepdims=True)\n",
    "        sigma = X.std(axis=0, keepdims=True)\n",
    "        sigma[np.equal(sigma,0)]=1.0 # avoid division by zero in case of degenerate features\n",
    "    else:\n",
    "        compute_stats = False\n",
    "        \n",
    "    #Normalize the data\n",
    "    X = (X-mu)/sigma\n",
    "    bias_feature = np.ones((X.shape[0],1))\n",
    "    X = np.concatenate([np.ones((X.shape[0],1)),X],1)\n",
    "\n",
    "    if visualize:\n",
    "        print('VISUALIZE AFTER NORMALIZATION')\n",
    "        #Visualize example after normalization\n",
    "        show_fMNIST_example(X[300])\n",
    "        show_fMNIST_example(X[500])\n",
    "    \n",
    "    if compute_stats:\n",
    "        return X, mu, sigma\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTr, yTr = read_fMNIST('Training')\n",
    "xTr, mu, sigma = preprocess_data(xTr,visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the main routine for our experiments: load training/validation data sets, and tune the regularization parameter (and perhaps other parameters you want to tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run minibatch SGD for every value of lambda and record training/val accuracy\n",
    "def tune_regularization(xTr,yTr,xVl,yVl,lambdas,sgd_opt):\n",
    "    \n",
    "    obj=dict()\n",
    "    trAcc=dict()\n",
    "    model=dict()\n",
    "    valAcc=dict()\n",
    "\n",
    "    for lmbda in lambdas:\n",
    "        model[lmbda], obj[lmbda],trAcc[lmbda] = minibatch_sgd(xTr,yTr,xVl,yVl,lmbda, sgd_opt['lr'],sgd_opt['maxepochs'],sgd_opt['bsize'],verbose=sgd_opt['verbose'])\n",
    "        valAcc[lmbda] = accuracy(xVl,model[lmbda],yVl)\n",
    "        if sgd_opt['verbose'] >= 0:\n",
    "            print('-------------- lambda=%.5f, val Acc = %.4f  (tr Acc = %.4f)'%(lmbda,valAcc[lmbda],trAcc[lmbda][-1]))\n",
    "        \n",
    "    return model, obj, trAcc, valAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTr, yTr = read_fMNIST('Training')\n",
    "xVl, yVl = read_fMNIST('Validation')\n",
    "\n",
    "xTr,mu,sigma = preprocess_data(xTr,visualize=False)\n",
    "xVl = preprocess_data(xVl,mu,sigma,visualize=False)\n",
    "\n",
    "# values below are a recommendation for initial exploration -- you should see if you want to refine these\n",
    "\n",
    "lambdas = [0,.0001,.001,.01,.1,1.0,10.0]\n",
    "sgd_opt = {'bsize':16, 'maxepochs':20, 'lr':.1,'verbose':0}\n",
    "\n",
    "\n",
    "model, obj, trAcc, valAcc = tune_regularization(xTr,yTr,xVl,yVl,lambdas,sgd_opt)\n",
    "\n",
    "*************** write code to select best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best model and use it to generate predictions for test data (to be submitted to Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTe = read_fMNIST('testing')\n",
    "xTe = preprocess_data(xTe,mu,sigma,visualize=False)\n",
    "\n",
    "\n",
    "#Decide which model you want to use for your submission\n",
    "chosen_model = model[best]\n",
    "\n",
    "#Make predictions and write them to a csv file\n",
    "final_preds = np.argmax(infer(xTe,chosen_model),axis=1)\n",
    "create_submission_file('./large_submission.csv', final_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will repeat this experiment with the small training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTr, yTr = read_fMNIST('Training', load_small=True)\n",
    "xVl, yVl = read_fMNIST('Validation')\n",
    "\n",
    "\n",
    "xTr,mu,sigma = preprocess_data(xTr,visualize=False)\n",
    "xVl = preprocess_data(xVl,mu,sigma,visualize=False)\n",
    "\n",
    "lambdas = [0,.0001,.001,.01,.1,1.0,10.0]\n",
    "sgd_opt = {'bsize':3, 'maxepochs':30, 'lr':.1,'verbose':0}\n",
    "\n",
    "\n",
    "model, obj, trAcc, valAcc = tune_regularization(xTr,yTr,xVl,yVl,lambdas,sgd_opt)\n",
    "\n",
    "*************** code to select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTe = read_fMNIST('testing')\n",
    "xTe = preprocess_data(xTe,mu,sigma,visualize=False)\n",
    "\n",
    "\n",
    "#Decide which model you want to use for your submission\n",
    "chosen_model = model[best]\n",
    "\n",
    "#Make predictions and write them to a csv file\n",
    "final_preds = np.argmax(infer(xTe,chosen_model),axis=1)\n",
    "create_submission_file('./small_submission.csv', final_preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
